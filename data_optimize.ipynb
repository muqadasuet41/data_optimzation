{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd1w8KNcNuCAB/SVrl2mLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muqadasuet41/data_optimzation/blob/main/data_optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "# Streamlit Skill Profiling Merger — works in Google Colab local tunnel and Streamlit Cloud\n",
        "# Usage: Upload multiple employee Excel files (from Cycle1 and Cycle2). Export final_master.xlsx.\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "st.set_page_config(page_title=\"Skill Profiling Merger\", layout=\"wide\")\n",
        "\n",
        "st.title(\"Skill Profiling — Auto Master File Generator\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "Upload all employee Excel files (Cycle 1 and Cycle 2).\n",
        "This app will:\n",
        "- Parse each employee file (supports row-wise or column-wise skill formats)\n",
        "- Detect cycle where possible (filename contains 'cycle1'/'cycle2' or dates)\n",
        "- Merge into Cycle1 / Cycle2 sheets and a Combined master sheet\n",
        "- Provide final_master.xlsx download\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def detect_cycle_from_filename(fn: str):\n",
        "    fn_low = fn.lower()\n",
        "    if \"cycle1\" in fn_low or \"c1\" in fn_low:\n",
        "        return 1\n",
        "    if \"cycle2\" in fn_low or \"c2\" in fn_low:\n",
        "        return 2\n",
        "    # try detect year-month or date in filename\n",
        "    date_match = re.search(r'(20\\d{2}[-_]\\d{1,2}[-_]\\d{1,2})', fn)\n",
        "    if date_match:\n",
        "        try:\n",
        "            dt = pd.to_datetime(date_match.group(1).replace('_','-'))\n",
        "            # heuristic: older -> cycle 1, newer -> cycle 2\n",
        "            # We can't be fully certain — return None to let merging logic handle.\n",
        "            return None\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def parse_excel_bytes(bytes_io, filename):\n",
        "    \"\"\"\n",
        "    Return DataFrame with columns: ['Employee','Skill','Level','Cycle']\n",
        "    Accepts many layouts:\n",
        "    - Long format: columns include 'Skill' and 'Level' (or similar)\n",
        "    - Wide format: first column maybe 'Skill' or row contains skills as columns (employee name as filename)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # try reading first sheet\n",
        "        xls = pd.read_excel(bytes_io, sheet_name=0, header=None)\n",
        "    except Exception as e:\n",
        "        # fallback: read with default pandas detection\n",
        "        bytes_io.seek(0)\n",
        "        df = pd.read_excel(bytes_io, sheet_name=0)\n",
        "        xls = df\n",
        "\n",
        "    # We'll try multiple heuristics\n",
        "    # 1) If it has header row with 'skill' and 'level'\n",
        "    bytes_io.seek(0)\n",
        "    df = pd.read_excel(bytes_io, sheet_name=0)\n",
        "    raw = df.copy()\n",
        "    df_cols = [str(c).strip().lower() for c in raw.columns]\n",
        "\n",
        "    employee_name = re.sub(r'\\.xlsx?$','', filename, flags=re.I)\n",
        "\n",
        "    # Heuristic A: explicit 'skill' and 'level' columns\n",
        "    skill_col = None\n",
        "    level_col = None\n",
        "    for c in raw.columns:\n",
        "        cname = str(c).strip().lower()\n",
        "        if cname in ['skill', 'skill name', 'skills', 'skill_title', 'competency', 'competency name']:\n",
        "            skill_col = c\n",
        "        if cname in ['level','skill level', 'rating','proficiency','score']:\n",
        "            level_col = c\n",
        "\n",
        "    rows = []\n",
        "    detected_cycle = detect_cycle_from_filename(filename)\n",
        "    if skill_col is not None and level_col is not None:\n",
        "        for _, r in raw.iterrows():\n",
        "            sk = r[skill_col]\n",
        "            lv = r[level_col]\n",
        "            if pd.isna(sk):\n",
        "                continue\n",
        "            try:\n",
        "                lv_val = int(lv)\n",
        "            except:\n",
        "                # attempt numeric casting\n",
        "                try:\n",
        "                    lv_val = int(float(lv))\n",
        "                except:\n",
        "                    lv_val = lv\n",
        "            rows.append({'Employee': employee_name, 'Skill': str(sk).strip(), 'Level': lv_val, 'Cycle': detected_cycle})\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    # Heuristic B: long format but header names different (first column skill, second level)\n",
        "    if raw.shape[1] >= 2:\n",
        "        # check whether first column looks like skill names and second numeric\n",
        "        first_col = raw.columns[0]\n",
        "        second_col = raw.columns[1]\n",
        "        # Count numeric-like in second column\n",
        "        numeric_count = pd.to_numeric(raw[second_col], errors='coerce').notna().sum()\n",
        "        if numeric_count >= max(1, int(0.3 * len(raw))):\n",
        "            for _, r in raw.iterrows():\n",
        "                sk = r[first_col]\n",
        "                lv = r[second_col]\n",
        "                if pd.isna(sk):\n",
        "                    continue\n",
        "                try:\n",
        "                    lv_val = int(lv)\n",
        "                except:\n",
        "                    try:\n",
        "                        lv_val = int(float(lv))\n",
        "                    except:\n",
        "                        lv_val = lv\n",
        "                rows.append({'Employee': employee_name, 'Skill': str(sk).strip(), 'Level': lv_val, 'Cycle': detected_cycle})\n",
        "            return pd.DataFrame(rows)\n",
        "\n",
        "    # Heuristic C: wide format where columns are skills and single row contains levels\n",
        "    # If columns are skill names and there are numeric entries under them, treat as one employee row (or multiple rows)\n",
        "    # We will iterate rows and find numeric-like values in many columns\n",
        "    candidates = []\n",
        "    for idx, r in raw.iterrows():\n",
        "        numeric_cells = 0\n",
        "        skill_values = {}\n",
        "        for col in raw.columns:\n",
        "            val = r[col]\n",
        "            if pd.isna(val):\n",
        "                continue\n",
        "            # treat as numeric skill level when small integer 0-5\n",
        "            try:\n",
        "                n = int(val)\n",
        "                if 0 <= n <= 10:\n",
        "                    numeric_cells += 1\n",
        "                    skill_values[str(col).strip()] = n\n",
        "            except:\n",
        "                # treat strings like '3' or '2'?\n",
        "                try:\n",
        "                    n = int(float(val))\n",
        "                    numeric_cells += 1\n",
        "                    skill_values[str(col).strip()] = n\n",
        "                except:\n",
        "                    pass\n",
        "        if numeric_cells >= 1:\n",
        "            # Make rows out of skill_values\n",
        "            for sk, lv in skill_values.items():\n",
        "                rows.append({'Employee': employee_name, 'Skill': sk, 'Level': lv, 'Cycle': detected_cycle})\n",
        "    if rows:\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    # Last fallback: If nothing detected well, try flattening all string cells as possible skill names with level NaN\n",
        "    for _, r in raw.iterrows():\n",
        "        for col in raw.columns:\n",
        "            val = r[col]\n",
        "            if pd.isna(val):\n",
        "                continue\n",
        "            s = str(val).strip()\n",
        "            if len(s) > 0 and len(s) < 80:\n",
        "                # keep as skill with unknown level\n",
        "                rows.append({'Employee': employee_name, 'Skill': s, 'Level': None, 'Cycle': detected_cycle})\n",
        "    if rows:\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    # if still nothing, return empty df\n",
        "    return pd.DataFrame(columns=['Employee','Skill','Level','Cycle'])\n",
        "\n",
        "\n",
        "def merge_cycles(dfs):\n",
        "    \"\"\"\n",
        "    dfs: list of dataframes each with Employee, Skill, Level, Cycle\n",
        "    Logic:\n",
        "    - Partition into cycle1 and cycle2 if cycle info available.\n",
        "    - For combined master: for same Employee+Skill use the value from Cycle2 if present; otherwise Cycle1; if multiple entries use max level as a fallback.\n",
        "    - Also produce Cycle1 sheet and Cycle2 sheet.\n",
        "    \"\"\"\n",
        "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "    # normalize skill strings\n",
        "    combined['Skill'] = combined['Skill'].astype(str).str.strip()\n",
        "    combined['Employee'] = combined['Employee'].astype(str).str.strip()\n",
        "\n",
        "    # Partition by Cycle info when available\n",
        "    df_c1 = combined[combined['Cycle']==1].copy()\n",
        "    df_c2 = combined[combined['Cycle']==2].copy()\n",
        "    # entries with no cycle info\n",
        "    df_unknown = combined[combined['Cycle'].isna()].copy()\n",
        "\n",
        "    # If no explicit cycle tags, try to infer by upload order cannot be done reliably here.\n",
        "    # We'll treat unknown as part of cycle2 (latest) if both cycles present overall.\n",
        "    if df_c1.empty and df_unknown.empty and not df_c2.empty:\n",
        "        df_c1 = pd.DataFrame(columns=combined.columns)\n",
        "    if df_c1.empty and df_c2.empty and not df_unknown.empty:\n",
        "        # all files unknown: treat all as cycle2 (latest)\n",
        "        df_c2 = df_unknown.copy()\n",
        "        df_unknown = pd.DataFrame(columns=combined.columns)\n",
        "\n",
        "    # if both c1 and c2 exist, keep unknown as c2 (assuming they are updates)\n",
        "    if not df_c1.empty and not df_c2.empty and not df_unknown.empty:\n",
        "        df_c2 = pd.concat([df_c2, df_unknown], ignore_index=True)\n",
        "        df_unknown = pd.DataFrame(columns=combined.columns)\n",
        "\n",
        "    # If only unknown exists, assign to cycle2\n",
        "    if df_c1.empty and df_c2.empty and not df_unknown.empty:\n",
        "        df_c2 = df_unknown.copy()\n",
        "        df_unknown = pd.DataFrame(columns=combined.columns)\n",
        "\n",
        "    # Create helper pivot per cycle: for each employee+skill, pick max level (if multiple rows)\n",
        "    def normalize_cycle_df(dfc):\n",
        "        if dfc.empty:\n",
        "            return dfc\n",
        "        # coerce numeric\n",
        "        dfc['Level_num'] = pd.to_numeric(dfc['Level'], errors='coerce')\n",
        "        dfc = dfc.sort_values(by=['Employee','Skill','Level_num'], ascending=[True,True,False])\n",
        "        # keep first (highest) level per employee+skill\n",
        "        dfc = dfc.groupby(['Employee','Skill'], as_index=False).first()[['Employee','Skill','Level_num']]\n",
        "        dfc.rename(columns={'Level_num':'Level'}, inplace=True)\n",
        "        return dfc\n",
        "\n",
        "    n1 = normalize_cycle_df(df_c1)\n",
        "    n2 = normalize_cycle_df(df_c2)\n",
        "\n",
        "    # Combined: start from n1, then overlay n2 values, and also include skills only in n2\n",
        "    combined_master = pd.concat([n1, n2], ignore_index=True)\n",
        "    # keep the highest-level per Employee+Skill; if duplicates, prefer n2 value by marking origin\n",
        "    # Use groupby and take max Level\n",
        "    combined_master['Level'] = pd.to_numeric(combined_master['Level'], errors='coerce')\n",
        "    combined_master = combined_master.groupby(['Employee','Skill'], as_index=False)['Level'].max()\n",
        "    combined_master = combined_master.sort_values(['Employee','Skill']).reset_index(drop=True)\n",
        "\n",
        "    return n1.sort_values(['Employee','Skill']), n2.sort_values(['Employee','Skill']), combined_master\n",
        "\n",
        "# === Streamlit upload UI ===\n",
        "st.sidebar.header(\"Upload & Options\")\n",
        "uploaded_files = st.sidebar.file_uploader(\"Upload employee Excel files (.xlsx .xls) — multiple\", accept_multiple_files=True, type=['xlsx','xls'])\n",
        "\n",
        "auto_filename = st.sidebar.text_input(\"Master output filename\", value=\"final_master.xlsx\")\n",
        "\n",
        "show_preview = st.sidebar.checkbox(\"Show parsed preview\", value=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    st.sidebar.markdown(f\"Uploaded files: {len(uploaded_files)}\")\n",
        "    parsed_dfs = []\n",
        "    parse_errors = []\n",
        "    for uf in uploaded_files:\n",
        "        try:\n",
        "            bytes_data = uf.read()\n",
        "            parsed = parse_excel_bytes(io.BytesIO(bytes_data), uf.name)\n",
        "            if parsed.empty:\n",
        "                parse_errors.append(uf.name)\n",
        "            else:\n",
        "                parsed_dfs.append(parsed)\n",
        "                if show_preview:\n",
        "                    with st.expander(f\"Preview parsed from {uf.name}\"):\n",
        "                        st.dataframe(parsed.head(200))\n",
        "        except Exception as e:\n",
        "            parse_errors.append(f\"{uf.name} : {str(e)}\")\n",
        "\n",
        "    if parse_errors:\n",
        "        st.sidebar.warning(\"Files that couldn't be parsed cleanly (open them to check format):\\n\" + \"\\n\".join(parse_errors))\n",
        "\n",
        "    if parsed_dfs:\n",
        "        st.success(\"Parsing complete. Now merging...\")\n",
        "        c1_df, c2_df, master_df = merge_cycles(parsed_dfs)\n",
        "\n",
        "        st.header(\"Master Summary\")\n",
        "        st.subheader(\"Combined Master (Employee - Skill - Level)\")\n",
        "        st.dataframe(master_df)\n",
        "\n",
        "        # Provide per-employee pivot option\n",
        "        st.subheader(\"Pivot: Employees as rows, Skills as columns\")\n",
        "        pivot = master_df.pivot_table(index='Employee', columns='Skill', values='Level', aggfunc='first').fillna('')\n",
        "        st.dataframe(pivot)\n",
        "\n",
        "        # Prepare excel to download\n",
        "        towrite = io.BytesIO()\n",
        "        with pd.ExcelWriter(towrite, engine='openpyxl') as writer:\n",
        "            c1_df.to_excel(writer, sheet_name='Cycle1', index=False)\n",
        "            c2_df.to_excel(writer, sheet_name='Cycle2', index=False)\n",
        "            master_df.to_excel(writer, sheet_name='Master_Combined', index=False)\n",
        "            pivot.to_excel(writer, sheet_name='Master_Pivot', index=True)\n",
        "        towrite.seek(0)\n",
        "\n",
        "        st.download_button(label=\"Download final master .xlsx\", data=towrite, file_name=auto_filename, mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
        "\n",
        "        # Also allow saving to server (useful in Colab)\n",
        "        if st.button(\"Save final_master.xlsx on server (app working dir)\"):\n",
        "            with open(auto_filename, \"wb\") as f:\n",
        "                f.write(towrite.read())\n",
        "            st.success(f\"Saved as {auto_filename} in current working directory.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Upload employee Excel files (multiple). If testing in Google Colab, use google.colab.files.upload to upload files to the notebook workspace and then use the Tunnel commands to run the Streamlit app.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc6rnpbv8tno",
        "outputId": "eecb3eaa-d9a2-4638-fa45-903aca0d5bcc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-10-20 09:28:39.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.231 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.232 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.237 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.239 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.240 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.241 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.242 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.243 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.244 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.244 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.246 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.248 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-20 09:28:39.249 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}